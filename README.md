Weather and Traffic Data PipelineThis project implements a simple ETL (Extract, Transform, Load) pipeline to fetch traffic and weather data, store it in a DuckDB database, and visualize the transformed data.Project StructureThe project is organized as follows:your_project_root/
├── ELTscripts/
│   ├── extract_traffic_duckdb.py   # Extracts and transforms traffic data
│   ├── load_traffic_duckdb.py      # Loads traffic data into DuckDB
│   ├── extract_weather_duckdb.py   # Extracts and loads weather data into DuckDB
│   └── transform_weather_traffic_duckdb.py # Transforms/joins weather and traffic data in DuckDB
├── requirements.txt              # Lists Python dependencies
├── run_full_pipeline.py          # Orchestrates the entire ETL and Visualization process
├── setup.ps1                     # PowerShell script for environment setup
├── view_duckdb_tables.py         # Python script to view DuckDB table contents
├── visualize_duckdb_data.py      # Python script to visualize transformed data
└── .env                          # Environment variables (API keys, DB path, etc.)
File Descriptionsrun_full_pipeline.py: This is the main script that orchestrates the entire data pipeline. It calls the functions from the ELT scripts to extract, load, and transform the data, and then runs the visualization script.ELTscripts/: This folder contains the individual scripts responsible for the Extract, Load, and Transform steps.extract_traffic_duckdb.py: Fetches traffic flow data from the TomTom Traffic API and performs initial transformation into a pandas DataFrame.load_traffic_duckdb.py: Contains a function to load pandas DataFrames into a specified DuckDB table.extract_weather_duckdb.py: Fetches current weather data from a weather API and loads it into a DuckDB table.transform_weather_traffic_duckdb.py: Connects to the DuckDB database and performs a SQL transformation to join weather and traffic data, aggregating traffic data by location.visualize_duckdb_data.py: This script queries the transformed data from DuckDB and generates visualizations (e.g., time series plots) using Plotly, saving the output to an HTML file.view_duckdb_tables.py: A utility script to connect to the DuckDB database and display the contents of the weather_data, traffic_flow_data, and transformed_weather_traffic tables.requirements.txt: Specifies the Python libraries required to run the project (e.g., pandas, duckdb, requests, plotly, python-dotenv).setup.ps1: A PowerShell script to automate the setup of the Python virtual environment and installation of dependencies listed in requirements.txt..env: This file (you'll need to create it) is used to store environment variables such as API keys and the DuckDB database path.Setup InstructionsFollow these steps to set up the project environment and get the pipeline running:Prerequisites:Ensure you have Python 3.7 or higher installed.Ensure you have PowerShell installed (standard on Windows).Ensure you have Git installed.Clone the Repository:Clone this repository to your local machine using Git.git clone <repository_url>
cd <repository_directory>
(Replace <repository_url> and <repository_directory> with the actual details).Run the Setup Script:Open PowerShell in the project's root directory and run the setup script. This will create a Python virtual environment (.niceTraffic) and install the necessary libraries..\setup.ps1
This script will also activate the virtual environment in your current PowerShell session. You should see (.niceTraffic) at the beginning of your PowerShell prompt, indicating the virtual environment is active.Create the .env file:Create a file named .env in the project's root directory. This file will store your API keys and database path. Add the following lines, replacing the placeholder values with your actual API keys:TOMTOM_API_KEY=YOUR_TOMTOM_API_KEY
WEATHER_API_KEY=YOUR_WEATHER_API_KEY
# Optional: Specify a custom path for the DuckDB database file
# DUCKDB_DATABASE_PATH=./data/my_traffic_data.duckdb
Replace YOUR_TOMTOM_API_KEY with your TomTom Traffic API key.Replace YOUR_WEATHER_API_KEY with your Weather API key.You can optionally uncomment and modify DUCKDB_DATABASE_PATH if you want to store the database file in a different location or with a different name.UsageOnce the setup is complete, you can run the pipeline and view the data.Run the Full Pipeline:With the virtual environment activated (you should see (.niceTraffic) in your prompt), run the main pipeline script:& .\run_full_pipeline.py
This script will perform the following steps:Extract traffic data for the configured points.Extract weather data for the configured locations.Load the raw data into traffic_flow_data and weather_data tables in traffic_data.duckdb.Transform the data by joining weather and traffic information and aggregating traffic data by location, storing the results in the transformed_weather_traffic table.Generate a visualization HTML file (weather_traffic_time_visualization.html) and attempt to open it in your browser.View DuckDB Tables:You can use the view_database.ps1 script to inspect the contents of the tables in your DuckDB database at any time. Ensure your virtual environment is activated before running it:.\view_database.ps1
This script will print the first few rows and the total row count for the weather_data, traffic_flow_data, and transformed_weather_traffic tables to your console.Accumulating DataNote that the transformed_weather_traffic table is currently configured to accumulate data from each pipeline run. Each time you run run_full_pipeline.py, new aggregated data for the configured locations will be appended to this table. The weather_data and traffic_flow_data tables also accumulate data from each run.If you ever need to start with a clean database, you can manually delete the traffic_data.duckdb file (or the file specified by DUCKDB_DATABASE_PATH in your .env). The pipeline will recreate it on the next run.